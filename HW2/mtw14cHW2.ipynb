{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Dr D  \n",
    "Class: Pattern Recognition Spring 2018\n",
    "\n",
    "# Predicting Titanic survival with a random forest classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "In this assignment we are given a sample set of data on demographics and survival of a large number of the passengers of the Titanic and we are asked to determine whether some other passengers would have survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Method\n",
    "\n",
    "We are limited to using decision trees or random forest classifiers for this problem. I am going to try to solve it using a random forest, partially because I think it will work better, but mostly because I have not built one before. A random forest classifier is a meta classifier which creates a number of decisions trees using randomely selected sub-sets of the original data set and randomly selected sub-sets of the features. It wouldn't be too hard to implement on our own (in theory), but it is already built in [SKLearn](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) so there is no reason to build it again. I am going to try to pull some information out of the forest to display how it is making its decisions to show that I know what it is doing, but it should be really easy to classify using it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "\n",
    "Here we load and pre-process all of the data to prepare it to be used to test and train classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>7.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  sex   age   fare\n",
       "0       3    1  42.0   7.55\n",
       "1       3    1  13.0  20.25\n",
       "2       3    1  16.0  20.25\n",
       "3       3    0  35.0  20.25\n",
       "4       3    0  16.0   7.65"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "df = pd.read_csv('titanic_sample_data.csv')\n",
    "ds = pd.read_csv('titanic_challenge.csv')\n",
    "# OK.  Enough messing around.  Let's get to work.\n",
    "# We need make an array X with just numeric features, and y with just the category (survived)\n",
    "X = df.drop(['survived','name'], axis = 1)\n",
    "y = df['survived']\n",
    "X['sex'] = X['sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Here we produce a random forest classifier. I went with 1000 trees in the forest arbitrarily, and ran some tests to determine an optimal depth. I also eventually settled on using entropy instead of Gini score to evaluate purity. \n",
    "\n",
    "The random forest classifier is just a container of a large number of decision trees, each made with a random subset of the training samples and of the available features. I just stuck with defaults, which means each tree gets to use two of the four features. Each tree gets a 'vote' on how to classify a new data point. I can not find information on how voting is handled or what the decision rule is. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=1,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building 1000 trees in the forest each with a max depth of 5. Also set to give verbose output and to parallel compute on all processors\n",
    "clf = RandomForestClassifier(n_estimators=1000,max_depth=5,verbose=1,n_jobs=-1,criterion = 'entropy')\n",
    "clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785992217899\n",
      "[ 0.13903435  0.47152513  0.17725675  0.21218377]\n"
     ]
    }
   ],
   "source": [
    "# it was real easy to train that forest. Let's see how it did.\n",
    "print(clf.score(X_test,y_test))\n",
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "# that's not bad. Let's see if we can try classifying \n",
    "from decimal import *\n",
    "name = ds['name']\n",
    "newX = ds.drop(['name'],axis=1)\n",
    "newX['sex'] = newX['sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "ans = pd.DataFrame()\n",
    "ans['name'] = name.values\n",
    "ans['survived?'] = clf.predict(newX)\n",
    "probs = clf.predict_proba(newX)\n",
    "ans['certainty'] = probs[:,1]\n",
    "# holy cow, pandas support for applying arbitrary lambda functions is awesome\n",
    "ans['survived?'] = ans['survived?'].apply(lambda x: \"yep\" if x else \"nope\")\n",
    "# certainty is calculated by normalizing probabilities from 0-.5 (inverse) and .5-1 to 0-100%\n",
    "ans['certainty'] = ans['certainty'].apply(lambda x: abs(x-.5)*200)\n",
    "ans['certainty'] = ans['certainty'].apply(lambda x: '%.1f%%' %  x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>survived?</th>\n",
       "      <th>certainty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>yep</td>\n",
       "      <td>22.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Astor, Mrs. John Jacob (Madeleine Talmadge Force)</td>\n",
       "      <td>yep</td>\n",
       "      <td>83.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baclini, Miss. Helene Barbara</td>\n",
       "      <td>yep</td>\n",
       "      <td>33.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Braund, Mr. Lewis Richard</td>\n",
       "      <td>nope</td>\n",
       "      <td>69.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carlsson, Mr. Frans Olof</td>\n",
       "      <td>nope</td>\n",
       "      <td>43.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cavendish, Mrs. Tyrell William (Julia Florence...</td>\n",
       "      <td>yep</td>\n",
       "      <td>79.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Frolicher-Stehli, Mr. Maxmillian</td>\n",
       "      <td>nope</td>\n",
       "      <td>73.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gracie, Col. Archibald IV</td>\n",
       "      <td>nope</td>\n",
       "      <td>32.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>yep</td>\n",
       "      <td>10.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Icard, Miss. Amelie</td>\n",
       "      <td>yep</td>\n",
       "      <td>94.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jalsevac, Mr. Ivan</td>\n",
       "      <td>nope</td>\n",
       "      <td>73.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kirkland, Rev. Charles Leonard</td>\n",
       "      <td>nope</td>\n",
       "      <td>87.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>nope</td>\n",
       "      <td>79.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nakid, Mr. Sahid</td>\n",
       "      <td>nope</td>\n",
       "      <td>64.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Robins, Mr. Alexander A</td>\n",
       "      <td>nope</td>\n",
       "      <td>79.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ross, Mr. John Hugo</td>\n",
       "      <td>nope</td>\n",
       "      <td>35.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sandstrom, Miss. Beatrice Irene</td>\n",
       "      <td>yep</td>\n",
       "      <td>26.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Seward, Mr. Frederic Kimber</td>\n",
       "      <td>nope</td>\n",
       "      <td>26.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Turpin, Mrs. William John Robert (Dorothy Ann ...</td>\n",
       "      <td>yep</td>\n",
       "      <td>74.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Daugherity, Michael</td>\n",
       "      <td>nope</td>\n",
       "      <td>49.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Byrd, Reuben</td>\n",
       "      <td>nope</td>\n",
       "      <td>71.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Carstens, Paul</td>\n",
       "      <td>nope</td>\n",
       "      <td>70.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mulder, Samuel</td>\n",
       "      <td>nope</td>\n",
       "      <td>49.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Shudde, Rachael</td>\n",
       "      <td>yep</td>\n",
       "      <td>80.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Shurtz, Kevin</td>\n",
       "      <td>nope</td>\n",
       "      <td>63.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ter Kuile, Anna</td>\n",
       "      <td>yep</td>\n",
       "      <td>83.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Werner, Preston</td>\n",
       "      <td>nope</td>\n",
       "      <td>67.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Williams, Matthew</td>\n",
       "      <td>nope</td>\n",
       "      <td>74.1%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name survived? certainty\n",
       "0                      Allison, Master. Hudson Trevor       yep     22.1%\n",
       "1   Astor, Mrs. John Jacob (Madeleine Talmadge Force)       yep     83.5%\n",
       "2                       Baclini, Miss. Helene Barbara       yep     33.4%\n",
       "3                           Braund, Mr. Lewis Richard      nope     69.0%\n",
       "4                            Carlsson, Mr. Frans Olof      nope     43.9%\n",
       "5   Cavendish, Mrs. Tyrell William (Julia Florence...       yep     79.3%\n",
       "6                    Frolicher-Stehli, Mr. Maxmillian      nope     73.5%\n",
       "7                           Gracie, Col. Archibald IV      nope     32.4%\n",
       "8        Hirvonen, Mrs. Alexander (Helga E Lindqvist)       yep     10.7%\n",
       "9                                 Icard, Miss. Amelie       yep     94.6%\n",
       "10                                 Jalsevac, Mr. Ivan      nope     73.0%\n",
       "11                     Kirkland, Rev. Charles Leonard      nope     87.1%\n",
       "12                              Montvila, Rev. Juozas      nope     79.8%\n",
       "13                                   Nakid, Mr. Sahid      nope     64.3%\n",
       "14                            Robins, Mr. Alexander A      nope     79.9%\n",
       "15                                Ross, Mr. John Hugo      nope     35.1%\n",
       "16                    Sandstrom, Miss. Beatrice Irene       yep     26.9%\n",
       "17                        Seward, Mr. Frederic Kimber      nope     26.5%\n",
       "18  Turpin, Mrs. William John Robert (Dorothy Ann ...       yep     74.8%\n",
       "19                                Daugherity, Michael      nope     49.2%\n",
       "20                                       Byrd, Reuben      nope     71.9%\n",
       "21                                     Carstens, Paul      nope     70.5%\n",
       "22                                     Mulder, Samuel      nope     49.4%\n",
       "23                                    Shudde, Rachael       yep     80.2%\n",
       "24                                      Shurtz, Kevin      nope     63.2%\n",
       "25                                    Ter Kuile, Anna       yep     83.7%\n",
       "26                                    Werner, Preston      nope     67.9%\n",
       "27                                  Williams, Matthew      nope     74.1%"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      yep\n",
       "1      yep\n",
       "2      yep\n",
       "3     nope\n",
       "4     nope\n",
       "5      yep\n",
       "6     nope\n",
       "7     nope\n",
       "8      yep\n",
       "9      yep\n",
       "10    nope\n",
       "11    nope\n",
       "12    nope\n",
       "13    nope\n",
       "14    nope\n",
       "15    nope\n",
       "16     yep\n",
       "17    nope\n",
       "18     yep\n",
       "19    nope\n",
       "20    nope\n",
       "21    nope\n",
       "22    nope\n",
       "23     yep\n",
       "24    nope\n",
       "25     yep\n",
       "26    nope\n",
       "27    nope\n",
       "Name: survived?, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans['survived?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here the expected result for each passenger. I have also attempted to approximate certainty of our predictions by mapping the score from 0.5->0 and 0.5->1 to 0->100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "I have no way to validate my results. I expect that they have about the same accuracy as the test data set, that is, 80-83%. According to my classifier, all of the guys in our class will die, but both of the girls will live. The classifier follows expected trends, that women and children are more likely to survive and higher class passengers are more likely to survive. \n",
    "\n",
    "I learned a considerable amount about pandas dataframes in working on this. It is a super helpful library. I learned about random forests, but there is really not that much to them since they just implement a number of decision trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
