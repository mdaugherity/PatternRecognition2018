{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Sam Mulder\n",
    "\n",
    "Class: Pattern Recognition Spring 2018\n",
    "\n",
    "# HW 2 \n",
    "Trees on the Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "1) We must create a classifying mechanism to determine who lives and who dies from the Titanic. Explain problem, how to solve, and what my results mean. 2) Make it easy to paste your predictions in a Google Spreadsheet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Method\n",
    "We will be allowed to use either a Decision Tree Classifier(DTC) or a Random Forest Classifier(RFC). It is very easy to simply run the RandomForestClassifier (RFC). Our goal is to 'twist' the knobs on its available parameters to achieve the most optimal result for the classifier to have the most optimal accuracy possible on average. I will also go about explaining the results and output the predicted data for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from IPython.display import Image as PImage\n",
    "from subprocess import check_call\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import re\n",
    "from sklearn import model_selection\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "data = pd.read_csv('titanic_sample_data.csv')\n",
    "data.describe()\n",
    "\n",
    "X = data.drop(['survived','name'], axis = 1)\n",
    "y = data['survived']\n",
    "X['sex'] = X['sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "X[:5]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of children who survived 51.65562913907284\n",
      "% of adults who survived 38.70967741935484\n",
      "% of females who survived 75.06561679790026\n",
      "% of males who survived 20.0\n"
     ]
    }
   ],
   "source": [
    "print('% of children who survived', 100*np.mean(data['survived'][data['age'] < 18]))\n",
    "print('% of adults who survived', 100*np.mean(data['survived'][data['age'] > 18]))\n",
    "print('% of females who survived', 100*np.mean(data['survived'][data['sex'] == 'female']))\n",
    "print('% of males who survived', 100*np.mean(data['survived'][data['sex'] == 'male']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first goal is to determine the most optimal values to attribute to the different parameters available on the FRC. Some of them will only be an intuitive argument and some will require testing along with intuition. \n",
    "\n",
    "Parameters available to choose from:\n",
    "n_estimators; \n",
    "criterion; \n",
    "max_features; \n",
    "max_depth; \n",
    "min_samples_split; \n",
    "min_samples_leaf; \n",
    "min_weight_fraction_leaf; \n",
    "max_leaf_nodes; \n",
    "min_impurity_split; \n",
    "min_impurity_decrease; \n",
    "bootstrap; \n",
    "oob_score; \n",
    "n_jobs; \n",
    "random_state; \n",
    "verbose; \n",
    "warm_start; \n",
    "class_weight; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82101167315175094"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the accuracy of our RFC without any parameter adjusmtents\n",
    "DTrfc = RandomForestClassifier()\n",
    "DTrfc.fit(X_train, y_train)\n",
    "DTrfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78599221789883267"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTdtc = DecisionTreeClassifier()\n",
    "DTdtc.fit(X_train, y_train)\n",
    "DTdtc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8443579766536965"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTbest = RandomForestClassifier(max_depth = 5, max_features = 4, criterion = 'entropy', n_estimators = 1000, min_samples_leaf = 3, verbose = 1)\n",
    "DTbest.fit(X_train, y_train)\n",
    "DTbest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23020467  0.42556306  0.18805202  0.15618025]\n"
     ]
    }
   ],
   "source": [
    "print(DTbest.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to describe why I chose these parameter definitions. I chose a max_depth = 5 based purely on empirical intuition. I ran the classifier many times and based on the score output I was able to generate the highest accuracy value on average when I set this parameter = 5. For max_features = 4 becuase this is the maximum number of features possible. I don't expect there to be much variability in this parameter becuase the most meaningful features will be used first so the less impactful ones will not have as great of an impact on the resulting tree. I set criterion = 'entropy' becuase of how they are defined. For one, I understand the definition of 'entropy' much better so it simply defines it as information gain. This parameter controls the split of each node and rather than using the 'gini impurity' we will use the 'entropy' criterion. I set the n_estimators = 1000 because this is a fair number that allows for a large amount trees to be created in the random forest however becuase of the structure of the random forest we do not risk overfitting. I chose min_samples_leaf = 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can also see the class importance according to the classifier built DTbest. It shows that 'sex' by far is the most important feature when determining the survival of the passenger and ticket class is next but not even close. This is an important thing to recognize to develop a full understanding of our model of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "In this section I will apply the RFC to the titanic_challenge.csv dataset to determine how effective this classifier is at the unknown dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pclass  sex   age  fare\n",
       "19       1    1  38.0   6.0\n",
       "20       2    1  21.0   8.0\n",
       "21       3    1  21.0  10.0\n",
       "22       1    1  20.0  12.0\n",
       "23       2    0  21.0  14.0\n",
       "24       3    1  21.0  16.0\n",
       "25       1    0  21.0  18.0\n",
       "26       2    1  22.0  20.0\n",
       "27       3    1  20.0  22.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = pd.read_csv('titanic_challenge.csv')\n",
    "X2 = data2.drop(['name'], axis = 1)\n",
    "X2['sex'] = X2['sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "X2[19:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "pred = DTbest.predict(X2)\n",
    "predprobraw = DTbest.predict_proba(X2)\n",
    "predprob = predprobraw[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we applied the RFC to the challenge dataset to calculate the predicted survival and the mean class probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name:</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Certainty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.933106</td>\n",
       "      <td>86.62%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Astor, Mrs. John Jacob (Madeleine Talmadge Force)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990340</td>\n",
       "      <td>98.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baclini, Miss. Helene Barbara</td>\n",
       "      <td>1</td>\n",
       "      <td>0.673236</td>\n",
       "      <td>34.65%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Braund, Mr. Lewis Richard</td>\n",
       "      <td>0</td>\n",
       "      <td>0.224595</td>\n",
       "      <td>55.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carlsson, Mr. Frans Olof</td>\n",
       "      <td>0</td>\n",
       "      <td>0.341712</td>\n",
       "      <td>31.66%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cavendish, Mrs. Tyrell William (Julia Florence...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.849237</td>\n",
       "      <td>69.85%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Frolicher-Stehli, Mr. Maxmillian</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088307</td>\n",
       "      <td>82.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gracie, Col. Archibald IV</td>\n",
       "      <td>0</td>\n",
       "      <td>0.352381</td>\n",
       "      <td>29.52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.439404</td>\n",
       "      <td>12.12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Icard, Miss. Amelie</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998213</td>\n",
       "      <td>99.64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jalsevac, Mr. Ivan</td>\n",
       "      <td>0</td>\n",
       "      <td>0.177828</td>\n",
       "      <td>64.43%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kirkland, Rev. Charles Leonard</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061051</td>\n",
       "      <td>87.79%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>0</td>\n",
       "      <td>0.165556</td>\n",
       "      <td>66.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nakid, Mr. Sahid</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140522</td>\n",
       "      <td>71.90%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Robins, Mr. Alexander A</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048285</td>\n",
       "      <td>90.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ross, Mr. John Hugo</td>\n",
       "      <td>0</td>\n",
       "      <td>0.463767</td>\n",
       "      <td>7.25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sandstrom, Miss. Beatrice Irene</td>\n",
       "      <td>1</td>\n",
       "      <td>0.653248</td>\n",
       "      <td>30.65%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Seward, Mr. Frederic Kimber</td>\n",
       "      <td>0</td>\n",
       "      <td>0.463526</td>\n",
       "      <td>7.29%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Turpin, Mrs. William John Robert (Dorothy Ann ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.897078</td>\n",
       "      <td>79.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Daugherity, Michael</td>\n",
       "      <td>0</td>\n",
       "      <td>0.259139</td>\n",
       "      <td>48.17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Byrd, Reuben</td>\n",
       "      <td>0</td>\n",
       "      <td>0.173983</td>\n",
       "      <td>65.20%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Carstens, Paul</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170579</td>\n",
       "      <td>65.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mulder, Samuel</td>\n",
       "      <td>0</td>\n",
       "      <td>0.258334</td>\n",
       "      <td>48.33%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Shudde, Rachael</td>\n",
       "      <td>1</td>\n",
       "      <td>0.899684</td>\n",
       "      <td>79.94%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Shurtz, Kevin</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140819</td>\n",
       "      <td>71.84%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ter Kuile, Anna</td>\n",
       "      <td>1</td>\n",
       "      <td>0.896474</td>\n",
       "      <td>79.29%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Werner, Preston</td>\n",
       "      <td>0</td>\n",
       "      <td>0.138282</td>\n",
       "      <td>72.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Williams, Matthew</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137733</td>\n",
       "      <td>72.45%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name:  Prediction  \\\n",
       "0                      Allison, Master. Hudson Trevor           1   \n",
       "1   Astor, Mrs. John Jacob (Madeleine Talmadge Force)           1   \n",
       "2                       Baclini, Miss. Helene Barbara           1   \n",
       "3                           Braund, Mr. Lewis Richard           0   \n",
       "4                            Carlsson, Mr. Frans Olof           0   \n",
       "5   Cavendish, Mrs. Tyrell William (Julia Florence...           1   \n",
       "6                    Frolicher-Stehli, Mr. Maxmillian           0   \n",
       "7                           Gracie, Col. Archibald IV           0   \n",
       "8        Hirvonen, Mrs. Alexander (Helga E Lindqvist)           0   \n",
       "9                                 Icard, Miss. Amelie           1   \n",
       "10                                 Jalsevac, Mr. Ivan           0   \n",
       "11                     Kirkland, Rev. Charles Leonard           0   \n",
       "12                              Montvila, Rev. Juozas           0   \n",
       "13                                   Nakid, Mr. Sahid           0   \n",
       "14                            Robins, Mr. Alexander A           0   \n",
       "15                                Ross, Mr. John Hugo           0   \n",
       "16                    Sandstrom, Miss. Beatrice Irene           1   \n",
       "17                        Seward, Mr. Frederic Kimber           0   \n",
       "18  Turpin, Mrs. William John Robert (Dorothy Ann ...           1   \n",
       "19                                Daugherity, Michael           0   \n",
       "20                                       Byrd, Reuben           0   \n",
       "21                                     Carstens, Paul           0   \n",
       "22                                     Mulder, Samuel           0   \n",
       "23                                    Shudde, Rachael           1   \n",
       "24                                      Shurtz, Kevin           0   \n",
       "25                                    Ter Kuile, Anna           1   \n",
       "26                                    Werner, Preston           0   \n",
       "27                                  Williams, Matthew           0   \n",
       "\n",
       "    Probability Certainty  \n",
       "0      0.933106    86.62%  \n",
       "1      0.990340    98.07%  \n",
       "2      0.673236    34.65%  \n",
       "3      0.224595    55.08%  \n",
       "4      0.341712    31.66%  \n",
       "5      0.849237    69.85%  \n",
       "6      0.088307    82.34%  \n",
       "7      0.352381    29.52%  \n",
       "8      0.439404    12.12%  \n",
       "9      0.998213    99.64%  \n",
       "10     0.177828    64.43%  \n",
       "11     0.061051    87.79%  \n",
       "12     0.165556    66.89%  \n",
       "13     0.140522    71.90%  \n",
       "14     0.048285    90.34%  \n",
       "15     0.463767     7.25%  \n",
       "16     0.653248    30.65%  \n",
       "17     0.463526     7.29%  \n",
       "18     0.897078    79.42%  \n",
       "19     0.259139    48.17%  \n",
       "20     0.173983    65.20%  \n",
       "21     0.170579    65.88%  \n",
       "22     0.258334    48.33%  \n",
       "23     0.899684    79.94%  \n",
       "24     0.140819    71.84%  \n",
       "25     0.896474    79.29%  \n",
       "26     0.138282    72.34%  \n",
       "27     0.137733    72.45%  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = data2['name']\n",
    "outtable = pd.DataFrame()\n",
    "outtable['Name:'] = names\n",
    "outtable['Prediction'] = pred\n",
    "outtable['Probability'] = predprob\n",
    "outtable['Certainty'] = predprob\n",
    "outtable['Certainty'] = outtable['Certainty'].apply(lambda x: abs(x-.5)*200)\n",
    "outtable['Certainty'] = outtable['Certainty'].apply(lambda x: '%.2f%%' %  x)\n",
    "outtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we needed to alter the values given to us by predict_proba becuase it returns the mean predicted class to create the certainty item for each passenger. The probability value is useful because it shows you essentially how often they received a 1/0 based on how close that value is to either 1/0. This is because it is the mean predicted class. The certainty I chose to show is useful because it gives a clear indication of how close the value is to the ideal state of 0/1 for its average depending on if we predict that person survives or dies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "Here we have produced a graph of each of the challenge data set passenegers predicted survival status. The row producing values of 1/0 represents the survival output with '1' being alive and '0' being dead. The row of data with continuous set of values from 0 - 1 represents the predicted class probabilities. This data was produced by using the random forest classifier built by pass the training data through the classifier.\n",
    "\n",
    "The RFC works by essentially created a 'forest' of decision trees that are based on choosing a random asortment of features and sample data points for each tree. There is some overlap based on the settings that we have but this is beneficial becuase it results in increased variability over the course of all 1000 decsion trees that we have it set to create. Once we have all of these trees created in our random forest, we simply passed more data points through it and reported the result for each new passenger (whether or not they lived or died).\n",
    "\n",
    "The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. The predicted class is the one with highest mean probability estimate across the trees.\n",
    "\n",
    "The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class in a leaf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
